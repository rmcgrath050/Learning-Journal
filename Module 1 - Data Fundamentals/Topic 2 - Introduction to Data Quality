Ideas for this mod:
- which data formats do lloyds deal with and show example here of how this is handled? eg CSV files , though stored procedure 
- Talk to Robbie about how data policies - Data access groups how these are set up within server , how different types of access works in business , what is process here
- Also raising a change in business (speak to Robbie ) might fall into different cat


Off the job learning:
-built archeiving table to receive data - but then i didnt built the arhieving procedure so not sure this counts? 
- does otf learning need to be brand new learning or something in day to day role ive done before? 
- Week 2  - Based on the insights gained from data profiling, the company established data quality rules and constraints to address the identified issues.  - Implementated primary keys 
on tables ? - however i do this as part of my role (preventing errors and duplicate entries in the database.)
- weeks 2 downloaded phython , followed correct documentation to install this



##What role do data engineers play in ensuring data quality?
Accuracy
Integrity
Consistency 
Timeliness


##Introducing open standards

They are like the rules of a game, designed so everyone plays by the same rules
Compatibility: They help different systems and tools talk to each other smoothly
Flexibility: You're not locked into using products from just one company
Innovation: Encourages new ideas and improvements by allowing more people to contribute
Cost-effective: Often free to use, which can save money on technology costs


Example of open standards include:
Industry-specific standards (e.g., HL7 in healthcare)
Cross-industry standards (e.g., Unicode, HTTP, TCP/IP)
Open data standards (e.g., OData, OpenAPI)



Banks, Financial, and Professional Services:
ISO 20022 for electronic data interchange in finance
FIX (Financial Information eXchange) protocol for real-time electronic exchange of securities transactions
XBRL (eXtensible Business Reporting Language) for financial reporting
ACORD (Association for Cooperative Operations Research and Development) standards for insurance industry


##The FAIR data standard:
Findable e.g assigning unique identifiers, creating a metadata catalogue
Accessible eg accessed by users and systems
Interoperable e.g integrated with other datasets and systems,
Reusability


##Data and Metadata Standards

Example incudes:
###Open Banking Standards -
Define technical specifications and APIs for securely sharing banking data between banks and
third-party service providers, promoting competition and innovation in financial services 

###UK Data Service - The UK Data Service is a repository of socioeconomic research data, 
promoting data standards such as sharing best practices across teams and data management frameworks


##DCMI
The Dublin Core Metadata Initiative (DCMI) offers a set of metadata standards used to describe resources in various domains. 
Its core elements provide a basic framework for describing digital resources such as documents, images, videos, and web pages. 


##Challenges of unstandardised data
Data silos: Without standardised formats and structures, data often becomes siloed within different departments or systems, hindering collaboration and intergrated analysis.
Inconsistencies: Lack of standards leads to inconsistencies in data representation, making it difficult to integrate and analyse data across different sources
Integration issues: Integrating data from disparate sources becomes challenging due to differences in formats, schemas, and semantics

These challenges are overcome though: 
- Facilitating data sharing eg though frameworks etc
- Promoting collaboration eg Standardised data formats and protocols enable seamless collaboration
- Enhancing interoperability eg allowing diverse systems to communicate and work together


#Lesson 3 : Navigating quality issues in XML data formats

##Structured and unstructured data
Structured - Data with a defined data type, format, and structure.
Semi Structured - Textual data files with a discernable pattern that allows them to be parsed easily. Examples include XMLs and JSONs. 
Unstructured - Data that has no inherent structure and is usually stored as different types of files such as text documents, PDFs, images, and video. 

Key factors to consider when choosing a data format: 
- Evaluate the intended purpose of the data and how it will be processed and analysed. 
- Consider the availability of tools and libraries that support the chosen data format
- Assess the performance characteristics of the data format, including efficiency in data storage (file size) , transmission, and processing. 
- Evaluate the interoperability of the data format across different systems, platforms, and programming languages. 
- Consider the data integrity and security features offered by the data format, such as built-in error detection and correction mechanisms, 
  encryption support, and access control mechanisms. 


 ##XML data format
XML is a more general-purpose language used for describing and structuring data in a wide range of applications, 
including web services, document management, and data exchange. Mastery of XML enables engineers to parse, transform, and 
integrate data seamlessly across diverse systems and platforms. 

1. Human and machine readable
2. Tagging - It uses tags to define elements within a document
3. Hierarchical structure - XML documents have a hierarchical structure with nested elements, similar to HTML but with more flexibility and extensibility

Pros include:
1. Human readable
2. Flexibility in representing diverse types of data
3. Interoperability: XML serves as a universal format for data exchange

Cons include:
1. Verbose syntax: XML documents can become overly complex and repetitive
2. Parsing overhead: Processing XML documents requires parsing, which can be computationally intensive
3. Lack of schema enforcement: XML does not enforce strict data validation rules by default, leading to potential issues with data quality and integrity

###Soultions:
Parsing errors:
To mitigate these issues, it's essential to validate XML documents against predefined schemas, use robust XML parsers, 
implement error handling mechanisms, sanitise data before parsing, and conduct regular testing and validation. 

Character encoding issues:
To mitigate character encoding issues in XML documents, several measures can be taken, including:
1. Standardise character encoding: Use a consistent character encoding scheme throughout XML documents, such as UTF-8 or UTF-16,
2. Encode special characters: Properly encode special characters
3. Validate encoding: Validate XML documents to ensure that character encoding declarations match the actual encoding used within the document.

Schema validation
Non-compliance with the schema can lead to data inconsistency, interoperability issues, and errors during data processing. 
Adhering to a defined schema helps maintain data consistency and ensures compatibility with other systems or applications that rely on the XML data.

#Lession 4 - Navigating quality issues in CSV data formats

 CSV serve as a standard format for exchanging data between different systems and platforms

Pros - Human readable, Simple Structure
Cons - Lack of standardisation, Limited data types + Fragility: They are susceptible to errors due to inconsistent use of delimiters and line terminators


Quality issue 1: Inconsistent formats
Delimiters, encapsulators, and line terminators are essential components in CSV files. 
Delimiters separate fields, encapsulators enclose fields with special characters, and line terminators indicate the end of a record.
Solution : To mitigate this challenge and data quality issue you can standardise the use of delimiters, encapsulators, and line terminators across all CSV files.

Quality issue 2 : Data type mismatch
The absence of explicit data types in CSV files can result in misinterpretation of data, leading to incorrect analysis. 
Solution : To mitigate this challenge and data quality issue you can provide metadata or schema information alongside CSV files to specify data types. 
You can also perform data validation and type casting during parsing to ensure consistency. 

Quality issue 3 : Header misalignment
Missing or misaligned headers in CSV files can lead to errors in data mapping and processing
Solution :  To mitigate this challenge and data quality issue its important to ensure that all CSV files have consistent headers and alignment. 
Implement error handling mechanisms to detect and rectify header misalignment issues.  


#Lesson 5 - Navigating quality issues in JSON data formats

JavaScript Object Notation (JSON), is a lightweight data interchange format that offers simplicity and efficiency.
We'll also address challenges like nesting complexity, key-value pair integrity, and efficient management of large JSON datasets.

JSON has become the preferred data format for web applications and Application Programming Interfaces (APIs) due to its simplicity, flexibility, 
and compatibility with JavaScript.
It is widely used for transmitting data between a server and a web application, making it an integral part of modern web development. 

An API is a set of protocols, routines, and tools that specify how software components should interact and communicate with each other. 

Pros - Human readable, lightweight (lighter file size) , and native JavaScript support
Cons:
1. Unlike XML, JSON does not have a built-in mechanism for defining a schema, which can lead to issues with data validation and consistency
2. Limited data types: JSON supports only a limited set of data types which may not be sufficient for all use cases
3. Nesting complexity: Complex nested structures in JSON can complicate parsing and processing, leading to performance issues


Issue 1: Nesting complexity
JSON allows for deeply nested structures, where objects can contain other objects or arrays of objects. 
While this flexibility is beneficial for representing complex data, it can also complicate parsing and increase processing time, especially for large datasets. 

issue 2: Key-value pair integrity
The key serves as an identifier for the associated value, allowing for easy retrieval and manipulation of data. 
Consistent key naming is essential in JSON to ensure the integrity and usability of the data.
must establish naming conventions and validation rules to maintain key-value pair integrity across JSON documents!

issue 3: Data volume
Data engineers must consider the scalability and performance implications of working with large JSON datasets and implement appropriate solutions to address these challenges.
Strategies for efficient processing include data streaming, pagination, compression, and distributed processing.


#Lesson 6 - A brief guide to responsible data handling

##Data policies
Data policies are formalised guidelines and procedures that govern how data is collected, managed, used, and protected within an organisation. 
Their primary purpose is to ensure that data is handled responsibly, ethically, and in compliance with relevant laws and industry standards.

Manged via:
Data access and usage policies
To safeguard data privacy and security, prevent unauthorised access or misuse, and ensure that data is used appropriately for legitimate business purposes


###Data retention and archiving policies 
where it should be stored, and when it should be securely disposed of or archived
Purpose: To manage storage resources efficiently, comply with legal and regulatory requirements, and minimise the risks associated with retaining unnecessary data

###Data sharing policies
These policies govern the sharing of data with external parties, such as partners, vendors, or regulatory authorities.
To establish guidelines for data exchange agreements, ensure compliance with data protection regulations, and protect intellectual property and sensitive information

A data sharing policy must consider consent, data subject rights, and data minimisation principles outlined in the General Data Protection Regulations (GDPR).

##Key Principles of GDPR
- Lawfulness, fairness and transparency 
- Purpose Limitation
- Data Minimisation
- Accuracy
- Accountability and Transparency 
- Storage limitation
- Integrity and confidentiality 


They must integrate consent management features into systems to enable easy withdrawal of consent and maintain compliant record-keeping practices.

#Leason 7 - Common strategies for ensuring data quality

##Examples of automated validation tools: 

Daily quality management Platforms 
Platforms like Informatica Data Quality, Talend Data Quality, and IBM InfoSphere Information Analyser offer comprehensive data quality management solutions
These platforms provide functionalities such as data profiling, cleansing, standardisation, and enrichment, 
enabling organisations to ensure the accuracy and consistency of their data.

Data Profiling Software
Tools like SAS Data Quality and Oracle Data Profiling enable organisations to analyse the structure, content, and quality characteristics of their data. 
These tools identify data anomalies, such as missing values, duplicate entries, and formatting errors, helping organisations
understand the overall quality of their data and prioritise areas for improvement.


Data Validation Frameworks
Frameworks like Apache Griffin and Great Expectations provide open-source solutions for validating and monitoring data quality. 
These frameworks offer customisable rules and metrics for assessing data quality, enabling organisations to define and enforce data quality standards according to their specific requirements.


##Benefits of Continuous Data Montioring 
Real-time anomaly detection - Continuous monitoring spots data anomalies instantly, averting data degradation and ensuring data reliability 
Proactive quality assurance - Monitoring data quality metrics in real-time lets organisations proactively address issues/failures before they disrupt live operations, maintaining data integrity 
Timely corrections - again minising disruption

##UDIS 
Definition: Unique identifiers (UIDs) are codes or numbers assigned to individual data records to distinguish them from others eg.primary keys 


#Lesson 8 - Data testing methodologies

###Horizontal Testing!
Consitancy around mutiple datasources,

###Historical analysis
Track changes in data quality over time and ensure the long-term integrity of the data. also measure trends 

###Rule Based testing
validation of data against predefined rules or criteria

###Statistical testing
identify anomalies and outliers within datasets using advanced statistical methods. 
eg. An e-commerce platform detecting fraudulent transactions based on purchasing patterns

Open-source libraries:
Griffin
pros: Cost-effective, community support, customisable.
cons: Limited features, may require technical expertise for customisation.

Informatica:
pros: Comprehensive features, user-friendly interfaces, company support.
cons: Costly licensing fees, dependency on vendor roadmap for updates.

manual data testing
1. Sampling: Selecting a representative subset of data for detailed analysis, allowing data analysts 
    to uncover potential issues without examining the entire dataset

2. Spot-checking: Randomly selecting individual data points or records to verify their accuracy and consistency 
    with expected values or patterns

3. Data validation through comparison with trusted sources: Cross-referencing data against authoritative or 
    verified sources to validate its accuracy 



##Summary

1. Data quality metrics, such as accuracy, integrity, consistency, and timeliness, are crucial for ensuring
    data meets technical and business requirements

2. Open standards and metadata play a vital role in promoting interoperability and data sharing across systems 
    and organisations

3. Navigating quality issues in common data formats like XML, CSV, and JSON is essential for seamless data
    exchange and integration

4. Data quality management strategies, including data profiling, validation, and monitoring, are necessary
    for maintaining data accuracy and reliability

5.  Adhering to data protection regulations, such as GDPR, and implementing robust data governance 
    frameworks are critical for responsible data handling

6. Data testing methodologies, including horizontal testing, historical analysis, rule-based testing, and statistical testing,
    help identify and mitigate data quality issues

7. Utilising data quality testing tools, frameworks, and manual testing techniques contributes
    to comprehensive data quality assurance







