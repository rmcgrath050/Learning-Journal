{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcgrath050/Learning-Journal/blob/main/python_hackathon_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0E1tNW4HDoM"
      },
      "source": [
        "# üöÄ Python Data Hackathon (Single-Notebook Edition)\n",
        "\n",
        "**Objectives**\n",
        "- Import one CSV and one log file into pandas\n",
        "- Clean (missing values, deduplicate)\n",
        "- Normalise (split into tidy tables with sensible keys)\n",
        "- Save outputs as CSVs\n",
        "- Light EDA (3 simple plots + 3 insights)\n",
        "- Refactor to run end-to-end in one go with functions + logging\n",
        "\n",
        "> **Tip:** Keep this notebook runnable top-to-bottom. Use the final **Run Pipeline** cell to test end-to-end.\n"
      ],
      "id": "l0E1tNW4HDoM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA6jS9WuHDoO"
      },
      "source": [
        "## üóìÔ∏è Day Plan (2 √ó 3 hours, incl. 20‚Äëmin break per block)\n",
        "\n",
        "**Block 1 (~3h)**\n",
        "1. Briefing & dataset walkthrough (20‚Äì25m)\n",
        "2. Ingest CSV + log ‚Üí Clean (nulls, duplicates) (65‚Äì75m)\n",
        "3. **Break (20m)**\n",
        "4. Normalise ‚Üí Save tidy tables (40‚Äì45m)\n",
        "5. Light EDA ‚Üí 3 plots + 3 insights (25‚Äì30m)\n",
        "\n",
        "**Block 2 (~3h)**\n",
        "1. Quick recap + quality bar (10‚Äì15m)\n",
        "2. Refactor to functions + logging + one-click run (80‚Äì90m)\n",
        "3. **Break (20m)**\n",
        "4. Polish: comments/docstrings, README notes, acceptance checks (35‚Äì45m)\n",
        "5. Demos/peer review (10‚Äì20m)\n"
      ],
      "id": "wA6jS9WuHDoO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529pcL9xHDoP"
      },
      "source": [
        "## ‚öôÔ∏è Setup: paths, logging, imports\n",
        "Use this once at the start. Re-run if you restart the kernel.\n"
      ],
      "id": "529pcL9xHDoP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SorR6PN0HDoQ",
        "outputId": "86e8f202-c1cc-4d89-92b6-48105a8291e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-24 22:25:36,979 | INFO | Setup complete. Folders ensured. Ready to start.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:hackathon:Setup complete. Folders ensured. Ready to start.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# ---- Parameters (edit as needed) ----\n",
        "#path functions create folders\n",
        "RAW_DIR = Path(\"data/raw\") # data folder\n",
        "PROC_DIR = Path(\"data/processed\") # processed sub folder in data\n",
        "\n",
        "LOG_DIR = Path(\"logs\") # logs folder.\n",
        "LOG_FILE = LOG_DIR / \"hackathon.log\" #hackathon sub folder\n",
        "\n",
        "REPORTS_DIR = Path(\"reports\") # reports folder\n",
        "FIGURES_DIR = REPORTS_DIR / \"figures\" # figures subfolder\n",
        "\n",
        "\n",
        "# Expected files (place in data/raw)\n",
        "CSV_FILE = RAW_DIR / \"orders.csv\"           # <- change as needed\n",
        "LOG_FILE_PATH = RAW_DIR / \"access.log\"      # <- change as needed\n",
        "\n",
        "# ---- Create folders (can also use bash: !mkdir -p data/raw data/processed logs reports/figures) ----\n",
        "for p in [RAW_DIR, PROC_DIR, LOG_DIR, REPORTS_DIR, FIGURES_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Logging config (file + console) ----\n",
        "logger = logging.getLogger(\"hackathon\")\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.handlers.clear()\n",
        "\n",
        "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "fh = logging.FileHandler(LOG_FILE, encoding='utf-8')\n",
        "fh.setFormatter(fmt)\n",
        "fh.setLevel(logging.INFO)\n",
        "logger.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler(sys.stdout)\n",
        "ch.setFormatter(fmt)\n",
        "ch.setLevel(logging.INFO)\n",
        "logger.addHandler(ch)\n",
        "\n",
        "logger.info(\"Setup complete. Folders ensured. Ready to start.\")\n"
      ],
      "id": "SorR6PN0HDoQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96X5iG6HDoR"
      },
      "source": [
        "## üß∞ Utilities (acceptance checks & helpers)\n",
        "You can extend these if helpful.\n"
      ],
      "id": "F96X5iG6HDoR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "-3GV7vmtHDoR"
      },
      "outputs": [],
      "source": [
        "def log_shape(df, name: str):\n",
        "    logger.info(f\"{name}: {df.shape[0]} rows √ó {df.shape[1]} cols\")\n",
        "\n",
        "def log_nulls(df, name: str):\n",
        "    null_pct = (df.isna().mean() * 100).round(2)\n",
        "    tops = null_pct.sort_values(ascending=False).head(5).to_dict()\n",
        "    logger.info(f\"{name}: top null% -> {tops}\")\n",
        "\n",
        "def assert_unique(df, cols, name=\"frame\"):\n",
        "    if df.duplicated(subset=cols).any():\n",
        "        dups = df[df.duplicated(subset=cols, keep=False)]\n",
        "        logger.error(f\"{name}: duplicates found on {cols}\")\n",
        "        raise AssertionError(f\"{name}: duplicates on {cols}\")\n",
        "    logger.info(f\"{name}: unique on {cols}\")\n",
        "\n",
        "def assert_no_orphans(child, parent, fk, pk, child_name=\"child\", parent_name=\"parent\"):\n",
        "    missing = ~child[fk].isin(parent[pk])\n",
        "    if missing.any():\n",
        "        n = int(missing.sum())\n",
        "        logger.error(f\"{child_name}: {n} orphan rows where {fk} not in {parent_name}.{pk}\")\n",
        "        raise AssertionError(f\"{child_name}: orphan FKs: {n}\")\n",
        "    logger.info(f\"{child_name}: no orphan {fk} against {parent_name}.{pk}\")\n",
        "\n",
        "def save_table(df, name):\n",
        "    out = PROC_DIR / f\"{name}.csv\"\n",
        "    df.to_csv(out, index=False)\n",
        "    logger.info(f\"Saved {name} -> {out}\")\n",
        "    return out\n",
        "\n",
        "def save_fig_current(name):\n",
        "    out = FIGURES_DIR / f\"{name}.png\"\n",
        "    plt.savefig(out, bbox_inches=\"tight\", dpi=150)\n",
        "    logger.info(f\"Saved figure -> {out}\")\n",
        "    return out\n"
      ],
      "id": "-3GV7vmtHDoR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTnesZHPHDoR"
      },
      "source": [
        "## üîΩ Block 1: Ingest ‚Üí Clean\n",
        "**Goal:** Read one CSV and one log file into pandas; handle nulls & duplicates with simple, defensible rules; log key metrics.\n"
      ],
      "id": "VTnesZHPHDoR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "79SE0RzEHDoS"
      },
      "outputs": [],
      "source": [
        "# --- 1.1 Load CSV ---\n",
        "def load_csv(path: Path) -> pd.DataFrame:\n",
        "    logger.info(f\"Loading CSV: {path}\")\n",
        "    df = pd.read_csv(path)\n",
        "    log_shape(df, \"raw_csv\")\n",
        "    log_nulls(df, \"raw_csv\")\n",
        "    return df\n",
        "\n",
        "try:\n",
        "    df_csv = load_csv(CSV_FILE)\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"CSV not found at {CSV_FILE}. Place your CSV in data/raw and re-run.\")\n",
        "    df_csv = pd.DataFrame()\n"
      ],
      "id": "79SE0RzEHDoS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "vhM_b-2YHDoS"
      },
      "outputs": [],
      "source": [
        "# --- 1.2 Load & parse log ---\n",
        "LOG_PATTERN = re.compile(\n",
        "    r\"^(?P<ip>\\S+)\\s+-\\s+-\\s+\\[(?P<ts>[^\\]]+)\\]\\s+\\\"(?P<method>\\S+)\\s+(?P<path>\\S+)\\s+(?P<proto>\\S+)\\\"\\s+(?P<status>\\d{3})\\s+(?P<size>\\S+)\"  # Apache-like\n",
        ")\n",
        "\n",
        "def parse_log_line(line: str):\n",
        "    m = LOG_PATTERN.search(line)\n",
        "    if not m:\n",
        "        return None\n",
        "    d = m.groupdict()\n",
        "    # quick cast\n",
        "    d[\"status\"] = int(d[\"status\"]) if d.get(\"status\") else None\n",
        "    d[\"size\"] = int(d[\"size\"]) if d.get(\"size\") and d[\"size\"].isdigit() else None\n",
        "    # timestamp parse (example format: 10/Oct/2000:13:55:36 -0700)\n",
        "    try:\n",
        "        d[\"ts\"] = datetime.strptime(d[\"ts\"], \"%d/%b/%Y:%H:%M:%S %z\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return d\n",
        "\n",
        "def load_log(path: Path) -> pd.DataFrame:\n",
        "    logger.info(f\"Loading log: {path}\")\n",
        "    rows = []\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for line in f:\n",
        "                d = parse_log_line(line)\n",
        "                if d:\n",
        "                    rows.append(d)\n",
        "        df = pd.DataFrame(rows)\n",
        "        log_shape(df, \"raw_log\")\n",
        "        log_nulls(df, \"raw_log\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Log not found at {path}. Place your log in data/raw and re-run.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "df_log = load_log(LOG_FILE_PATH)\n"
      ],
      "id": "vhM_b-2YHDoS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKuCwYyAHDoS"
      },
      "source": [
        "### üßΩ Cleaning functions\n",
        "Adjust rules as appropriate for your dataset. Be explicit in decisions.\n"
      ],
      "id": "PKuCwYyAHDoS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "QNvgCMNUHDoS"
      },
      "outputs": [],
      "source": [
        "def clean_frame(df: pd.DataFrame, *, key_cols=None, fill_rules=None, drop_na_thresh=None, name=\"frame\") -> pd.DataFrame:\n",
        "    \"\"\"Generic cleaner: dedupe on key_cols; fill using fill_rules; optional dropna threshold.\"\"\"\n",
        "    if df.empty:\n",
        "        logger.warning(f\"{name}: empty frame passed to clean_frame\")\n",
        "        return df.copy()\n",
        "    before = len(df)\n",
        "    if key_cols:\n",
        "        df = df.drop_duplicates(subset=key_cols)\n",
        "    else:\n",
        "        df = df.drop_duplicates()\n",
        "    d_removed = before - len(df)\n",
        "    if d_removed:\n",
        "        logger.info(f\"{name}: removed {d_removed} duplicates\")\n",
        "\n",
        "    # Fill rules: {'col': value or function}\n",
        "    fill_rules = fill_rules or {}\n",
        "    for col, val in fill_rules.items():\n",
        "        if callable(val):\n",
        "            df[col] = df[col].apply(lambda x: val(x))\n",
        "        else:\n",
        "            df[col] = df[col].fillna(val)\n",
        "\n",
        "    if drop_na_thresh is not None:\n",
        "        # Drop rows with too many NAs\n",
        "        df = df.dropna(thresh=drop_na_thresh)\n",
        "\n",
        "    log_shape(df, f\"clean_{name}\")\n",
        "    log_nulls(df, f\"clean_{name}\")\n",
        "    return df\n",
        "\n",
        "# Example usage (customise):\n",
        "df_clean = clean_frame(\n",
        "    df_csv,\n",
        "    key_cols=[\"order_id\"] if \"order_id\" in df_csv.columns else None,\n",
        "    fill_rules={},\n",
        "    drop_na_thresh=None,\n",
        "    name=\"csv\"\n",
        ")\n"
      ],
      "id": "QNvgCMNUHDoS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EndcrAYhHDoT"
      },
      "source": [
        "## üîÄ Normalise ‚Üí tidy tables\n",
        "Split into entities (e.g., users, products, orders, order_items). Adjust to your schema. Validate with unique PKs and non-orphan FKs.\n"
      ],
      "id": "EndcrAYhHDoT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "mNBaiDfZHDoT"
      },
      "outputs": [],
      "source": [
        "def to_tidy_tables(df: pd.DataFrame):\n",
        "    \"\"\"Example normalisation for an orders-like CSV with columns such as:\n",
        "    order_id, user_id, user_name, product_id, product_name, quantity, unit_price, order_ts\n",
        "    Adjust for your dataset. Returns dict of DataFrames.\n",
        "    \"\"\"\n",
        "    tables = {}\n",
        "    if df.empty:\n",
        "        return tables\n",
        "\n",
        "    cols = df.columns.str.lower().tolist()\n",
        "\n",
        "    # Users\n",
        "    user_cols = [c for c in cols if c.startswith(\"user_\") or c == \"user_id\"]\n",
        "    if \"user_id\" in cols:\n",
        "        users = (\n",
        "            df[[c for c in df.columns if c.lower() in set(user_cols)]]\n",
        "            .drop_duplicates()\n",
        "            .rename(columns=lambda c: c.lower())\n",
        "        )\n",
        "        assert_unique(users, [\"user_id\"], name=\"users\")\n",
        "        tables[\"users\"] = users\n",
        "\n",
        "    # Products\n",
        "    prod_cols = [c for c in cols if c.startswith(\"product_\") or c == \"product_id\"]\n",
        "    if \"product_id\" in cols:\n",
        "        products = (\n",
        "            df[[c for c in df.columns if c.lower() in set(prod_cols)]]\n",
        "            .drop_duplicates()\n",
        "            .rename(columns=lambda c: c.lower())\n",
        "        )\n",
        "        assert_unique(products, [\"product_id\"], name=\"products\")\n",
        "        tables[\"products\"] = products\n",
        "\n",
        "    # Orders\n",
        "    if \"order_id\" in cols:\n",
        "        order_cols = [c for c in df.columns if c.lower() in {\"order_id\", \"user_id\", \"order_ts\", \"order_date\"}]\n",
        "        orders = df[order_cols].drop_duplicates().rename(columns=lambda c: c.lower())\n",
        "        assert_unique(orders, [\"order_id\"], name=\"orders\")\n",
        "        if \"users\" in tables:\n",
        "            assert_no_orphans(orders, tables[\"users\"], \"user_id\", \"user_id\", child_name=\"orders\", parent_name=\"users\")\n",
        "        tables[\"orders\"] = orders\n",
        "\n",
        "    # Order items\n",
        "    if set([\"order_id\", \"product_id\"]).issubset(set(cols)):\n",
        "        oi_cols = [c for c in df.columns if c.lower() in {\"order_id\", \"product_id\", \"quantity\", \"unit_price\"}]\n",
        "        order_items = df[oi_cols].rename(columns=lambda c: c.lower())\n",
        "        # create a surrogate key if not present\n",
        "        order_items = order_items.copy()\n",
        "        order_items.insert(0, \"order_item_id\", range(1, len(order_items) + 1))\n",
        "        assert_no_orphans(order_items, tables.get(\"orders\", df[[\"order_id\"]].drop_duplicates().rename(columns=lambda c: c.lower())),\n",
        "                         \"order_id\", \"order_id\", child_name=\"order_items\", parent_name=\"orders\")\n",
        "        if \"products\" in tables:\n",
        "            assert_no_orphans(order_items, tables[\"products\"], \"product_id\", \"product_id\", child_name=\"order_items\", parent_name=\"products\")\n",
        "        tables[\"order_items\"] = order_items\n",
        "\n",
        "    return tables\n",
        "\n",
        "tables = to_tidy_tables(df_clean)\n",
        "for name, tdf in tables.items():\n",
        "    save_table(tdf, name)\n"
      ],
      "id": "mNBaiDfZHDoT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGJQtJvIHDoT"
      },
      "source": [
        "## üìà Light EDA (save figures + write 3 insights)\n",
        "Create at least 3 basic plots (histogram, bar/column, line). Save images under `reports/figures/` and summarise insights in `reports/EDA_summary.md`.\n"
      ],
      "id": "zGJQtJvIHDoT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "wnQ1PCz3HDoU"
      },
      "outputs": [],
      "source": [
        "# Example EDA skeleton ‚Äî adjust columns to your dataset\n",
        "EDA_NOTES = REPORTS_DIR / \"EDA_summary.md\"\n",
        "\n",
        "def run_eda(df: pd.DataFrame):\n",
        "    if df.empty:\n",
        "        logger.warning(\"EDA skipped: empty frame\")\n",
        "        return\n",
        "\n",
        "    # Histogram example\n",
        "    if \"unit_price\" in df.columns:\n",
        "        df[\"unit_price\"].dropna().plot(kind=\"hist\", bins=20)\n",
        "        plt.title(\"Distribution of unit_price\")\n",
        "        plt.xlabel(\"unit_price\")\n",
        "        save_fig_current(\"hist_unit_price\")\n",
        "        plt.close()\n",
        "\n",
        "    # Bar/column example\n",
        "    if set([\"product_id\", \"quantity\"]).issubset(df.columns):\n",
        "        (df.groupby(\"product_id\")[\"quantity\"].sum().sort_values().tail(10)).plot(kind=\"bar\")\n",
        "        plt.title(\"Top 10 products by quantity\")\n",
        "        plt.xlabel(\"product_id\")\n",
        "        plt.ylabel(\"total quantity\")\n",
        "        save_fig_current(\"bar_top_products\")\n",
        "        plt.close()\n",
        "\n",
        "    # Line example\n",
        "    for time_col in [\"order_ts\", \"order_date\", \"ts\"]:\n",
        "        if time_col in df.columns:\n",
        "            ts = df.dropna(subset=[time_col]).copy()\n",
        "            ts[time_col] = pd.to_datetime(ts[time_col], errors=\"coerce\")\n",
        "            g = ts.groupby(pd.Grouper(key=time_col, freq=\"D\")).size()\n",
        "            if len(g) > 0:\n",
        "                g.plot(kind=\"line\")\n",
        "                plt.title(f\"Daily counts by {time_col}\")\n",
        "                plt.xlabel(\"date\")\n",
        "                plt.ylabel(\"count\")\n",
        "                save_fig_current(f\"line_daily_counts_{time_col}\")\n",
        "                plt.close()\n",
        "                break\n",
        "\n",
        "    # Write 3 insights (edit these to be data-specific)\n",
        "    EDA_NOTES.write_text(\n",
        "        \"\"\"\n",
        "# EDA Summary (edit me)\n",
        "\n",
        "1) Describe a pattern you see in the histogram (e.g., skew/right-tail in unit_price).\n",
        "2) Describe the top categories/products and a possible reason.\n",
        "3) Describe the temporal trend (e.g., weekly cycles or anomalies).\n",
        "        \"\"\".strip(),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    logger.info(f\"Wrote EDA notes -> {EDA_NOTES}\")\n"
      ],
      "id": "wnQ1PCz3HDoU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBxnvN-cHDoU"
      },
      "source": [
        "## üß© Block 2: Refactor ‚Üí one-click run\n",
        "Wrap steps into functions and a simple orchestrator. Use the **Run Pipeline** cell below to execute end-to-end.\n"
      ],
      "id": "lBxnvN-cHDoU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "3SHgUtjFHDoU"
      },
      "outputs": [],
      "source": [
        "def run_pipeline():\n",
        "    logger.info(\"===== PIPELINE START =====\")\n",
        "    # Ingest\n",
        "    csv_df = load_csv(CSV_FILE)\n",
        "    log_df = load_log(LOG_FILE_PATH)\n",
        "\n",
        "    # Clean (customise rules/keys as needed)\n",
        "    clean_df = clean_frame(\n",
        "        csv_df,\n",
        "        key_cols=[\"order_id\"] if \"order_id\" in csv_df.columns else None,\n",
        "        fill_rules={},\n",
        "        name=\"csv\"\n",
        "    )\n",
        "\n",
        "    # Normalise\n",
        "    tidy = to_tidy_tables(clean_df)\n",
        "    for name, tdf in tidy.items():\n",
        "        save_table(tdf, name)\n",
        "\n",
        "    # EDA\n",
        "    run_eda(clean_df)\n",
        "\n",
        "    logger.info(\"===== PIPELINE COMPLETE =====\")\n",
        "    return {k: v.shape for k, v in tidy.items()}\n"
      ],
      "id": "3SHgUtjFHDoU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "aLO90XH6HDoU"
      },
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è Run Pipeline (edit file names/columns earlier as needed, then run this cell)\n",
        "shapes = run_pipeline()\n",
        "shapes\n"
      ],
      "id": "aLO90XH6HDoU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeNj3_KrHDoU"
      },
      "source": [
        "## ‚úÖ Definition of Done (DoD)\n",
        "- Notebook runs top-to-bottom without error.\n",
        "- `data/processed/` contains normalised tables (CSV).\n",
        "- `reports/figures/` contains 3 saved plots.\n",
        "- `logs/hackathon.log` records steps + metrics (rows in/out, null %, duplicates removed).\n",
        "- `reports/EDA_summary.md` has 3 concise insights.\n",
        "- Code has functions, clear comments, and meaningful names.\n"
      ],
      "id": "DeNj3_KrHDoU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrB6fXQTHDoV"
      },
      "source": [
        "## ü§ù Peer Review Checklist (10‚Äì15 min)\n",
        "- Can I run your notebook without touching cell order?\n",
        "- Are assumptions/rules written down (e.g., how nulls were handled)?\n",
        "- Are keys/relationships sensible? Any orphan checks?\n",
        "- Do the plots have titles/labels and are they saved?\n",
        "- Is the log file informative?\n"
      ],
      "id": "WrB6fXQTHDoV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPLLQBoXHDoV"
      },
      "source": [
        "## üåü Stretch Goals\n",
        "- Add simple unit tests (e.g., assert dedupe works) in a separate test cell.\n",
        "- Support config via a small dictionary or JSON file read.\n",
        "- Add `argparse`-style parameters using `papermill` tags (optional) or a top **Parameters** cell.\n",
        "- Add a data dictionary under `reports/`.\n"
      ],
      "id": "UPLLQBoXHDoV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}